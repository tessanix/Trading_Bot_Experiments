{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow_probability==0.17\n",
    "%pip install plotly\n",
    "%pip install -U kaleido\n",
    "%pip install tdqm\n",
    "%pip install pandas-ta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as pta\n",
    "import sys; sys.path.insert(1, '../../../../')\n",
    "from utils import utility \n",
    "\n",
    "pricesDataFolder = \"../../../../prices_data/\"\n",
    "\n",
    "dfH4 = pd.read_csv(\n",
    "    filepath_or_buffer=pricesDataFolder+\"H4/[SP500]_H4_2014-03-20__2024-03-20.csv\",\n",
    "    delimiter=\"\\t\"\n",
    ")\n",
    "\n",
    "dfH4 = dfH4.rename(\n",
    "    columns = {\n",
    "        '<CLOSE>':'close',\n",
    "        '<OPEN>':'open', \n",
    "        '<HIGH>':'high', \n",
    "        '<LOW>':'low'\n",
    "    }\n",
    ")\n",
    "\n",
    "dfH4['datetime'] = pd.to_datetime(dfH4['<DATE>'] + ' ' + dfH4['<TIME>'])\n",
    "dfH4 = dfH4.drop(['<TICKVOL>', '<VOL>', '<SPREAD>', '<DATE>', '<TIME>'], axis=1)\n",
    "dfH4 = utility.heikinashi(dfH4) #add heikin ashi candles\n",
    "dfStoch = pta.stoch(high=dfH4['high'], low=dfH4['low'], close=dfH4['close'])\n",
    "# print(dfStoch.head())\n",
    "dfH4[\"Stoch_k\"] = dfStoch['STOCHk_14_3_3'] \n",
    "dfH4[\"Stoch_d\"] = dfStoch['STOCHd_14_3_3'] \n",
    "\n",
    "dfH4[\"shortTermMA\"] = dfH4[\"close\"].rolling(window=50).mean() # add moving average 50\n",
    "dfH4[\"longTermMA\"] = dfH4[\"close\"].rolling(window=200).mean() # add moving average 200\n",
    "\n",
    "dfH4 = dfH4.iloc[200:]\n",
    "dfH4 = dfH4.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>datetime</th>\n",
       "      <th>HA close</th>\n",
       "      <th>HA open</th>\n",
       "      <th>HA high</th>\n",
       "      <th>HA low</th>\n",
       "      <th>shortTermMA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1858.90</td>\n",
       "      <td>1861.50</td>\n",
       "      <td>1854.90</td>\n",
       "      <td>1856.90</td>\n",
       "      <td>2014-03-20 00:00:00</td>\n",
       "      <td>1858.0500</td>\n",
       "      <td>1857.900</td>\n",
       "      <td>1861.50</td>\n",
       "      <td>1854.90</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1856.70</td>\n",
       "      <td>1858.40</td>\n",
       "      <td>1853.20</td>\n",
       "      <td>1856.10</td>\n",
       "      <td>2014-03-20 04:00:00</td>\n",
       "      <td>1856.1000</td>\n",
       "      <td>1857.900</td>\n",
       "      <td>1858.40</td>\n",
       "      <td>1853.20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1856.10</td>\n",
       "      <td>1860.80</td>\n",
       "      <td>1854.00</td>\n",
       "      <td>1857.80</td>\n",
       "      <td>2014-03-20 08:00:00</td>\n",
       "      <td>1857.1750</td>\n",
       "      <td>1856.400</td>\n",
       "      <td>1860.80</td>\n",
       "      <td>1854.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1857.80</td>\n",
       "      <td>1860.30</td>\n",
       "      <td>1852.70</td>\n",
       "      <td>1857.80</td>\n",
       "      <td>2014-03-20 12:00:00</td>\n",
       "      <td>1857.1500</td>\n",
       "      <td>1856.950</td>\n",
       "      <td>1860.30</td>\n",
       "      <td>1852.70</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1857.60</td>\n",
       "      <td>1873.60</td>\n",
       "      <td>1855.60</td>\n",
       "      <td>1872.60</td>\n",
       "      <td>2014-03-20 16:00:00</td>\n",
       "      <td>1864.8500</td>\n",
       "      <td>1857.800</td>\n",
       "      <td>1873.60</td>\n",
       "      <td>1855.60</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15421</th>\n",
       "      <td>5147.69</td>\n",
       "      <td>5154.58</td>\n",
       "      <td>5137.61</td>\n",
       "      <td>5138.60</td>\n",
       "      <td>2024-03-19 08:00:00</td>\n",
       "      <td>5144.6200</td>\n",
       "      <td>5144.425</td>\n",
       "      <td>5154.58</td>\n",
       "      <td>5137.61</td>\n",
       "      <td>5144.6264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15422</th>\n",
       "      <td>5138.84</td>\n",
       "      <td>5143.79</td>\n",
       "      <td>5122.34</td>\n",
       "      <td>5136.94</td>\n",
       "      <td>2024-03-19 12:00:00</td>\n",
       "      <td>5135.4775</td>\n",
       "      <td>5143.145</td>\n",
       "      <td>5143.79</td>\n",
       "      <td>5122.34</td>\n",
       "      <td>5145.4218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15423</th>\n",
       "      <td>5136.69</td>\n",
       "      <td>5175.13</td>\n",
       "      <td>5132.38</td>\n",
       "      <td>5170.60</td>\n",
       "      <td>2024-03-19 16:00:00</td>\n",
       "      <td>5153.7000</td>\n",
       "      <td>5137.890</td>\n",
       "      <td>5175.13</td>\n",
       "      <td>5132.38</td>\n",
       "      <td>5146.6928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15424</th>\n",
       "      <td>5170.85</td>\n",
       "      <td>5180.54</td>\n",
       "      <td>5163.05</td>\n",
       "      <td>5176.05</td>\n",
       "      <td>2024-03-19 20:00:00</td>\n",
       "      <td>5172.6225</td>\n",
       "      <td>5153.645</td>\n",
       "      <td>5180.54</td>\n",
       "      <td>5163.05</td>\n",
       "      <td>5147.6494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15425</th>\n",
       "      <td>5175.16</td>\n",
       "      <td>5176.18</td>\n",
       "      <td>5168.64</td>\n",
       "      <td>5170.13</td>\n",
       "      <td>2024-03-20 00:00:00</td>\n",
       "      <td>5172.5275</td>\n",
       "      <td>5173.450</td>\n",
       "      <td>5176.18</td>\n",
       "      <td>5168.64</td>\n",
       "      <td>5148.0044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15426 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          open     high      low    close            datetime   HA close  \\\n",
       "0      1858.90  1861.50  1854.90  1856.90 2014-03-20 00:00:00  1858.0500   \n",
       "1      1856.70  1858.40  1853.20  1856.10 2014-03-20 04:00:00  1856.1000   \n",
       "2      1856.10  1860.80  1854.00  1857.80 2014-03-20 08:00:00  1857.1750   \n",
       "3      1857.80  1860.30  1852.70  1857.80 2014-03-20 12:00:00  1857.1500   \n",
       "4      1857.60  1873.60  1855.60  1872.60 2014-03-20 16:00:00  1864.8500   \n",
       "...        ...      ...      ...      ...                 ...        ...   \n",
       "15421  5147.69  5154.58  5137.61  5138.60 2024-03-19 08:00:00  5144.6200   \n",
       "15422  5138.84  5143.79  5122.34  5136.94 2024-03-19 12:00:00  5135.4775   \n",
       "15423  5136.69  5175.13  5132.38  5170.60 2024-03-19 16:00:00  5153.7000   \n",
       "15424  5170.85  5180.54  5163.05  5176.05 2024-03-19 20:00:00  5172.6225   \n",
       "15425  5175.16  5176.18  5168.64  5170.13 2024-03-20 00:00:00  5172.5275   \n",
       "\n",
       "        HA open  HA high   HA low  shortTermMA  \n",
       "0      1857.900  1861.50  1854.90          NaN  \n",
       "1      1857.900  1858.40  1853.20          NaN  \n",
       "2      1856.400  1860.80  1854.00          NaN  \n",
       "3      1856.950  1860.30  1852.70          NaN  \n",
       "4      1857.800  1873.60  1855.60          NaN  \n",
       "...         ...      ...      ...          ...  \n",
       "15421  5144.425  5154.58  5137.61    5144.6264  \n",
       "15422  5143.145  5143.79  5122.34    5145.4218  \n",
       "15423  5137.890  5175.13  5132.38    5146.6928  \n",
       "15424  5153.645  5180.54  5163.05    5147.6494  \n",
       "15425  5173.450  5176.18  5168.64    5148.0044  \n",
       "\n",
       "[15426 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfH4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define the percentage split\n",
    "percentage = 0.3\n",
    "candlesWindow = 100\n",
    "maxTradeRange = 200\n",
    "\n",
    "dfH4['month'] = dfH4['datetime'].dt.month\n",
    "dfH4['year'] = dfH4['datetime'].dt.year\n",
    "# Group by year and month\n",
    "# ON ne prends pas tout le dataframe car un trade a besoin \n",
    "# de voir les candlesWindow en arrière et un trade peut durer maxTradeRange\n",
    "grouped = dfH4.iloc[candlesWindow:-maxTradeRange].groupby(['year', 'month'])\n",
    "\n",
    "# Initialize lists to store indices\n",
    "indices_30_percent = []\n",
    "indices_70_percent = []\n",
    "\n",
    "# Iterate over groups\n",
    "for name, group in grouped:\n",
    "    # print('name: ', name, 'group: ', group)\n",
    "    # Calculate the number of indices for 30% and 70%\n",
    "    num_indices = len(group)\n",
    "    num_indices_30 = int(np.ceil(num_indices * percentage))\n",
    "    num_indices_70 = num_indices - num_indices_30\n",
    "    \n",
    "    # Shuffle indices for each months to ensure randomness for month period\n",
    "    shuffled_indices = np.random.permutation(num_indices)\n",
    "    \n",
    "    # Split shuffled indices into 30% and 70%\n",
    "    indices_30 = shuffled_indices[:num_indices_30]\n",
    "    indices_70 = shuffled_indices[num_indices_30:]\n",
    "    \n",
    "    # Add indices to the lists\n",
    "    indices_30_percent.extend(group.iloc[indices_30].index)\n",
    "    indices_70_percent.extend(group.iloc[indices_70].index)\n",
    "\n",
    "# Now indices_30_percent contains 30% of indices from each month,\n",
    "# and indices_70_percent contains the remaining 70% of indices from each month\n",
    "\n",
    "indices_30_percent = np.random.permutation(indices_30_percent)\n",
    "indices_70_percent = np.random.permutation(indices_70_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3827, 7548, 6019, ..., 9498, 3033, 1652])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(indices_70_percent)\n",
    "print(indices_30_percent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of 4593 + 10533 = 15126\n",
      "len of dataframe is 15426\n"
     ]
    }
   ],
   "source": [
    "print(f'sum of {len(indices_30_percent)} + {len(indices_70_percent)} = {len(indices_30_percent)+len(indices_70_percent)}')\n",
    "print(f'len of dataframe is {len(dfH4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No common indices found between the two lists.\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any common indices between the two lists\n",
    "common_indices = set(indices_30_percent) & set(indices_70_percent)\n",
    "\n",
    "if len(common_indices) == 0:\n",
    "    print(\"No common indices found between the two lists.\")\n",
    "else:\n",
    "    print(\"Common indices found between the two lists.\")\n",
    "    print(\"Common indices:\", common_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dfH4[[\"close\", \"open\",\"high\", \"low\", \"shortTermMA\", \"HA close\",\"HA open\", \"HA high\", \"HA low\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>shortTermMA</th>\n",
       "      <th>HA close</th>\n",
       "      <th>HA open</th>\n",
       "      <th>HA high</th>\n",
       "      <th>HA low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1856.90</td>\n",
       "      <td>1858.90</td>\n",
       "      <td>1861.50</td>\n",
       "      <td>1854.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1858.0500</td>\n",
       "      <td>1857.900</td>\n",
       "      <td>1861.50</td>\n",
       "      <td>1854.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1856.10</td>\n",
       "      <td>1856.70</td>\n",
       "      <td>1858.40</td>\n",
       "      <td>1853.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1856.1000</td>\n",
       "      <td>1857.900</td>\n",
       "      <td>1858.40</td>\n",
       "      <td>1853.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1857.80</td>\n",
       "      <td>1856.10</td>\n",
       "      <td>1860.80</td>\n",
       "      <td>1854.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1857.1750</td>\n",
       "      <td>1856.400</td>\n",
       "      <td>1860.80</td>\n",
       "      <td>1854.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1857.80</td>\n",
       "      <td>1857.80</td>\n",
       "      <td>1860.30</td>\n",
       "      <td>1852.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1857.1500</td>\n",
       "      <td>1856.950</td>\n",
       "      <td>1860.30</td>\n",
       "      <td>1852.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1872.60</td>\n",
       "      <td>1857.60</td>\n",
       "      <td>1873.60</td>\n",
       "      <td>1855.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1864.8500</td>\n",
       "      <td>1857.800</td>\n",
       "      <td>1873.60</td>\n",
       "      <td>1855.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15421</th>\n",
       "      <td>5138.60</td>\n",
       "      <td>5147.69</td>\n",
       "      <td>5154.58</td>\n",
       "      <td>5137.61</td>\n",
       "      <td>5144.6264</td>\n",
       "      <td>5144.6200</td>\n",
       "      <td>5144.425</td>\n",
       "      <td>5154.58</td>\n",
       "      <td>5137.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15422</th>\n",
       "      <td>5136.94</td>\n",
       "      <td>5138.84</td>\n",
       "      <td>5143.79</td>\n",
       "      <td>5122.34</td>\n",
       "      <td>5145.4218</td>\n",
       "      <td>5135.4775</td>\n",
       "      <td>5143.145</td>\n",
       "      <td>5143.79</td>\n",
       "      <td>5122.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15423</th>\n",
       "      <td>5170.60</td>\n",
       "      <td>5136.69</td>\n",
       "      <td>5175.13</td>\n",
       "      <td>5132.38</td>\n",
       "      <td>5146.6928</td>\n",
       "      <td>5153.7000</td>\n",
       "      <td>5137.890</td>\n",
       "      <td>5175.13</td>\n",
       "      <td>5132.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15424</th>\n",
       "      <td>5176.05</td>\n",
       "      <td>5170.85</td>\n",
       "      <td>5180.54</td>\n",
       "      <td>5163.05</td>\n",
       "      <td>5147.6494</td>\n",
       "      <td>5172.6225</td>\n",
       "      <td>5153.645</td>\n",
       "      <td>5180.54</td>\n",
       "      <td>5163.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15425</th>\n",
       "      <td>5170.13</td>\n",
       "      <td>5175.16</td>\n",
       "      <td>5176.18</td>\n",
       "      <td>5168.64</td>\n",
       "      <td>5148.0044</td>\n",
       "      <td>5172.5275</td>\n",
       "      <td>5173.450</td>\n",
       "      <td>5176.18</td>\n",
       "      <td>5168.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15426 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         close     open     high      low  shortTermMA   HA close   HA open  \\\n",
       "0      1856.90  1858.90  1861.50  1854.90          NaN  1858.0500  1857.900   \n",
       "1      1856.10  1856.70  1858.40  1853.20          NaN  1856.1000  1857.900   \n",
       "2      1857.80  1856.10  1860.80  1854.00          NaN  1857.1750  1856.400   \n",
       "3      1857.80  1857.80  1860.30  1852.70          NaN  1857.1500  1856.950   \n",
       "4      1872.60  1857.60  1873.60  1855.60          NaN  1864.8500  1857.800   \n",
       "...        ...      ...      ...      ...          ...        ...       ...   \n",
       "15421  5138.60  5147.69  5154.58  5137.61    5144.6264  5144.6200  5144.425   \n",
       "15422  5136.94  5138.84  5143.79  5122.34    5145.4218  5135.4775  5143.145   \n",
       "15423  5170.60  5136.69  5175.13  5132.38    5146.6928  5153.7000  5137.890   \n",
       "15424  5176.05  5170.85  5180.54  5163.05    5147.6494  5172.6225  5153.645   \n",
       "15425  5170.13  5175.16  5176.18  5168.64    5148.0044  5172.5275  5173.450   \n",
       "\n",
       "       HA high   HA low  \n",
       "0      1861.50  1854.90  \n",
       "1      1858.40  1853.20  \n",
       "2      1860.80  1854.00  \n",
       "3      1860.30  1852.70  \n",
       "4      1873.60  1855.60  \n",
       "...        ...      ...  \n",
       "15421  5154.58  5137.61  \n",
       "15422  5143.79  5122.34  \n",
       "15423  5175.13  5132.38  \n",
       "15424  5180.54  5163.05  \n",
       "15425  5176.18  5168.64  \n",
       "\n",
       "[15426 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 18:40:29.359576: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-28 18:40:30.482115: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-04-28 18:40:31.745366: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-28 18:40:31.932144: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-28 18:40:31.932238: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-28 18:40:32.143155: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-28 18:40:32.143251: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-28 18:40:32.143262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-04-28 18:40:32.143300: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-28 18:40:32.143831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5558 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tfv1\n",
    "\n",
    "config = tfv1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tfv1.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO parameters tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 18:52:40.497879: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-28 18:52:41.195134: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-04-28 18:52:42.286746: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-28 18:52:42.317648: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-28 18:52:42.317698: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-28 18:52:42.322172: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-28 18:52:42.322264: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-28 18:52:42.322305: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-28 18:52:42.524735: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-28 18:52:42.524837: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-28 18:52:42.524850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-04-28 18:52:42.524891: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-28 18:52:42.524920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5558 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- loading models weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tessan/.local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 174 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving model weights ---\n",
      "episode:0, capital:4035.17, actions:[0, 2, 1], rewards:[45.1, 0.0, 35.2], avg_score:80.280, nb iter:3\n",
      "episode:1, capital:4032.47, actions:[0, 2, 1], rewards:[0.8, 0.0, -2.7], avg_score:39.190, nb iter:3\n",
      "episode:2, capital:4042.46, actions:[0, 1], rewards:[17.8, 10.0], avg_score:35.393, nb iter:2\n",
      "episode:3, capital:4047.49, actions:[0, 1], rewards:[-2.2, 5.0], avg_score:27.245, nb iter:2\n",
      "episode:4, capital:4079.29, actions:[0, 2, 2, 1], rewards:[21.6, 0.0, 0.0, 31.8], avg_score:32.476, nb iter:4\n",
      "episode:5, capital:4011.42, actions:[0, 2, 2, 1], rewards:[-46.5, 0.0, 0.0, -67.9], avg_score:7.995, nb iter:4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, base_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../tmp/\u001b[39m\u001b[38;5;124m'\u001b[39m, chkpt_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_last_trained\u001b[39m\u001b[38;5;124m'\u001b[39m, transParams\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m      9\u001b[0m agent\u001b[38;5;241m.\u001b[39mload_weights()\n\u001b[0;32m---> 10\u001b[0m score_history \u001b[38;5;241m=\u001b[39m \u001b[43mactorCritictrainingLoop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_games\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/Trading_bot_and_RL/reinforcement_learning/trading_agent/ppo/using_array/../../../../reinforcement_learning/trading_agent/ppo/using_array/ac_training_loop_without_strat.py:106\u001b[0m, in \u001b[0;36mactorCritictrainingLoop\u001b[0;34m(data, agent, pipValue, n_games, capital, save_model)\u001b[0m\n\u001b[1;32m    103\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore_transition(observation, maxSlInPips, entryPrice, action, prob, val, reward, done)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_steps \u001b[38;5;241m%\u001b[39m N \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# agent.learn()\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     learn_iters \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    109\u001b[0m observation \u001b[38;5;241m=\u001b[39m observation_\n",
      "File \u001b[0;32m~/dev/Trading_bot_and_RL/reinforcement_learning/trading_agent/ppo/using_array/../../../../reinforcement_learning/trading_agent/ppo/agent_no_strat.py:122\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m         critic_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mtrainable_variables\n\u001b[1;32m    121\u001b[0m         critic_grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(critic_loss, critic_params)\n\u001b[0;32m--> 122\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mactor_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(critic_grads, critic_params))\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mclear_memory()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:269\u001b[0m, in \u001b[0;36mBaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[1;32m    268\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:330\u001b[0m, in \u001b[0;36mBaseOptimizer.apply\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:380\u001b[0m, in \u001b[0;36mBaseOptimizer._backend_apply_gradients\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    373\u001b[0m     ops\u001b[38;5;241m.\u001b[39mcond(\n\u001b[1;32m    374\u001b[0m         is_update_step,\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: _update_step_fn(\u001b[38;5;28mself\u001b[39m, grads, trainable_variables),\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: _grad_accumulation_fn(\u001b[38;5;28mself\u001b[39m, grads),\n\u001b[1;32m    377\u001b[0m     )\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# Run udpate step.\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_update_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_variables_moving_average(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_variables\n\u001b[1;32m    387\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/optimizer.py:117\u001b[0m, in \u001b[0;36mTFOptimizer._backend_update_step\u001b[0;34m(self, grads, trainable_variables, learning_rate)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_backend_update_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads, trainable_variables, learning_rate):\n\u001b[1;32m    113\u001b[0m     trainable_variables \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    114\u001b[0m         v\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, backend\u001b[38;5;241m.\u001b[39mVariable) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m trainable_variables\n\u001b[1;32m    116\u001b[0m     ]\n\u001b[0;32m--> 117\u001b[0m     \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_merge_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_tf_update_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/distribute/merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[0;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m distribute_lib\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[1;32m     54\u001b[0m       fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/optimizer.py:131\u001b[0m, in \u001b[0;36mTFOptimizer._distributed_tf_update_step\u001b[0;34m(self, distribution, grads_and_vars, learning_rate)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step(grad, var, learning_rate)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[0;32m--> 131\u001b[0m     \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:3005\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3002\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   3003\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   3004\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 3005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3007\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[1;32m   3008\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:4075\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   4072\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[1;32m   4073\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[1;32m   4074\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[0;32m-> 4075\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:4081\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   4077\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[1;32m   4078\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[1;32m   4079\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[1;32m   4080\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[0;32m-> 4081\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4082\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[1;32m   4083\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:596\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    595\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/optimizer.py:128\u001b[0m, in \u001b[0;36mTFOptimizer._distributed_tf_update_step.<locals>.apply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_grad_to_update_var\u001b[39m(var, grad):\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/optimizers/adam.py:144\u001b[0m, in \u001b[0;36mAdam.update_step\u001b[0;34m(self, gradient, variable, learning_rate)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign(v_hat, ops\u001b[38;5;241m.\u001b[39mmaximum(v_hat, v))\n\u001b[1;32m    140\u001b[0m     v \u001b[38;5;241m=\u001b[39m v_hat\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign_sub(\n\u001b[1;32m    142\u001b[0m     variable,\n\u001b[1;32m    143\u001b[0m     ops\u001b[38;5;241m.\u001b[39mdivide(\n\u001b[0;32m--> 144\u001b[0m         ops\u001b[38;5;241m.\u001b[39mmultiply(m, alpha), ops\u001b[38;5;241m.\u001b[39madd(\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon)\n\u001b[1;32m    145\u001b[0m     ),\n\u001b[1;32m    146\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/ops/numpy.py:5731\u001b[0m, in \u001b[0;36msqrt\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   5729\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Sqrt()\u001b[38;5;241m.\u001b[39msymbolic_call(x)\n\u001b[1;32m   5730\u001b[0m x \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m-> 5731\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/sparse.py:385\u001b[0m, in \u001b[0;36melementwise_unary.<locals>.sparse_wrapper\u001b[0;34m(x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mIndexedSlices(\n\u001b[1;32m    382\u001b[0m         func(x\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), x\u001b[38;5;241m.\u001b[39mindices, x\u001b[38;5;241m.\u001b[39mdense_shape\n\u001b[1;32m    383\u001b[0m     )\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/numpy.py:1910\u001b[0m, in \u001b[0;36msqrt\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1904\u001b[0m x \u001b[38;5;241m=\u001b[39m convert_to_tensor(x)\n\u001b[1;32m   1905\u001b[0m dtype \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1906\u001b[0m     config\u001b[38;5;241m.\u001b[39mfloatx()\n\u001b[1;32m   1907\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m standardize_dtype(x\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1908\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39mresult_type(x\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m   1909\u001b[0m )\n\u001b[0;32m-> 1910\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39msqrt(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py:1010\u001b[0m, in \u001b[0;36mcast\u001b[0;34m(x, dtype, name)\u001b[0m\n\u001b[1;32m   1004\u001b[0m   x \u001b[38;5;241m=\u001b[39m indexed_slices\u001b[38;5;241m.\u001b[39mIndexedSlices(values_cast, x\u001b[38;5;241m.\u001b[39mindices, x\u001b[38;5;241m.\u001b[39mdense_shape)\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1006\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): If x is not already a Tensor, we could return\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m   \u001b[38;5;66;03m# ops.convert_to_tensor(x, dtype=dtype, ...)  here, but that\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m   \u001b[38;5;66;03m# allows some conversions that cast() can't do, e.g. casting numbers to\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m   \u001b[38;5;66;03m# strings.\u001b[39;00m\n\u001b[0;32m-> 1010\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1011\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_complex \u001b[38;5;129;01mand\u001b[39;00m base_type\u001b[38;5;241m.\u001b[39mis_floating:\n\u001b[1;32m   1012\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1013\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are casting an input of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1014\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_type\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.  This will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1015\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscard the imaginary part and may not be what you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1016\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintended.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1017\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:713\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[1;32m    712\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[0;32m--> 713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:209\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    207\u001b[0m overload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__tf_tensor__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m overload \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moverload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#  pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m base_type, conversion_func \u001b[38;5;129;01min\u001b[39;00m get(\u001b[38;5;28mtype\u001b[39m(value)):\n\u001b[1;32m    212\u001b[0m   \u001b[38;5;66;03m# If dtype is None but preferred_dtype is not None, we try to\u001b[39;00m\n\u001b[1;32m    213\u001b[0m   \u001b[38;5;66;03m# cast to preferred_dtype first.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py:64\u001b[0m, in \u001b[0;36mVariable.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion.py:161\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m\u001b[38;5;241m.\u001b[39mtf_export(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m     97\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[1;32m     99\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    100\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tensor_lib\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion.py:171\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# preferred_dtype = preferred_dtype or dtype_hint\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:242\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, core\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    240\u001b[0m   to_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__tf_tensor__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    241\u001b[0m   ret \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 242\u001b[0m       \u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#  pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    243\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m to_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    244\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m ret\n\u001b[1;32m    245\u001b[0m   )\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, accepted_result_types):\n\u001b[1;32m    247\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    248\u001b[0m       _add_error_prefix(\n\u001b[1;32m    249\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m returned non-Tensor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    251\u001b[0m           name\u001b[38;5;241m=\u001b[39mname))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:608\u001b[0m, in \u001b[0;36m_EagerTensorBase.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    603\u001b[0m         _add_error_prefix(\n\u001b[1;32m    604\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to capture an EagerTensor without \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilding a function.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    606\u001b[0m             name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    607\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mcapture(\u001b[38;5;28mself\u001b[39m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m--> 608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__tf_tensor__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/tensor.py:757\u001b[0m, in \u001b[0;36mTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    754\u001b[0m   spec \u001b[38;5;241m=\u001b[39m TensorSpec(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype)\n\u001b[1;32m    755\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m spec\n\u001b[0;32m--> 757\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28mself\u001b[39m, dtype: Optional[dtypes\u001b[38;5;241m.\u001b[39mDType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    759\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    760\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mis_compatible_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    762\u001b[0m         _add_error_prefix(\n\u001b[1;32m    763\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor conversion requested dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    764\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor Tensor with dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    765\u001b[0m             name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys; sys.path.insert(1, '../../../../')\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import utility\n",
    "from reinforcement_learning.trading_agent.ppo.using_array.ppo_training_loop_without_strat_with_idx import PPOtrainingLoop\n",
    "from reinforcement_learning.trading_agent.ppo.agent_no_strat import Agent\n",
    "\n",
    "\n",
    "gammaList = [0.7, 0.8, 0.9]\n",
    "gae_lambdaList = [0.85, 0.95]\n",
    "policy_clipList = [0.1, 0.05]\n",
    "nb_tunning_iterations = len(gammaList)*len(policy_clipList)*len(gae_lambdaList)\n",
    "\n",
    "\n",
    "combinations_already_tested = [(0.7, 0.85, 0.1), (0.7, 0.85, 0.05), (0.7, 0.95, 0.1),\n",
    "                               (0.7, 0.95, 0.05), (0.8, 0.85, 0.1), #(0.8, 0.85, 0.05),\n",
    "                               (0.8, 0.95, 0.1), (0.8, 0.95, 0.05), (0.9, 0.85, 0.1), \n",
    "                               (0.9, 0.85, 0.05), (0.9, 0.95, 0.1), (0.9, 0.95, 0.05)]\n",
    "\n",
    "d_model, d_ff, n = 12, 128, 4\n",
    "\n",
    "params = {'sequence_length':100, 'h':4, 'd_k':1, 'd_v':1, 'd_model':d_model, 'd_ff':d_ff, 'n':n, 'rate':0.2}\n",
    "model_version = f'model_{d_model}_{d_ff}_{n}'\n",
    "\n",
    "base_dir = '../tmp/'+model_version+'/'\n",
    "n_games=13_000\n",
    "\n",
    "if not os.path.exists(base_dir):\n",
    "    os.mkdir(base_dir)\n",
    "\n",
    "with tqdm(total=nb_tunning_iterations) as pbar:\n",
    "    for gamma in gammaList:\n",
    "        for gae_lambda in gae_lambdaList:\n",
    "            for policy_clip in policy_clipList:\n",
    "                \n",
    "                combination = (gamma, gae_lambda, policy_clip)\n",
    "                if combination not in combinations_already_tested:\n",
    "                    print(combination, \"not already tested!\")\n",
    "\n",
    "                    chkpt_dir = f'{gamma}_{gae_lambda}_{policy_clip}/'\n",
    "\n",
    "                    if not os.path.exists(base_dir+chkpt_dir):\n",
    "                        # shutil.rmtree(base_dir+chkpt_dir)\n",
    "                        os.mkdir(base_dir+chkpt_dir)\n",
    "\n",
    "                    agent = Agent(base_dir=base_dir, chkpt_dir=chkpt_dir, transParams=params,\n",
    "                                alpha=0.0001, gamma=gamma, gae_lambda=gae_lambda, policy_clip=policy_clip, batch_size=64, n_epochs=10)\n",
    "\n",
    "                    scores, capitals = PPOtrainingLoop(data, indices_70_percent, agent, n_games=n_games, save_model=True, d_model=d_model)\n",
    "                    utility.write_image_of_capital_and_scores(base_dir+chkpt_dir+'results.png', capitals, scores)\n",
    "                    print(f\"folder {chkpt_dir} updated!\")\n",
    "                else:\n",
    "                    print(combination, \"already tested!\")\n",
    "\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model parameters tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(1, '../../../../')\n",
    "import os\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import utility\n",
    "from reinforcement_learning.trading_agent.ppo.using_array.ppo_training_loop_without_strat_with_idx import PPOtrainingLoop\n",
    "from reinforcement_learning.trading_agent.ppo.agent_no_strat import Agent\n",
    "\n",
    "\n",
    "gamma       = 0.8\n",
    "gae_lambda  = 0.85\n",
    "policy_clip = 0.05\n",
    "\n",
    "d_modelList = [4]\n",
    "d_ffList    = [128, 256, 512]\n",
    "nList       = [4, 5, 6]\n",
    "\n",
    "nb_tunning_iterations = len(d_modelList)*len(d_ffList)*len(nList)\n",
    "\n",
    "n_games=10_000\n",
    "\n",
    "\n",
    "combinations_already_tested = [(4, 128, 4), (4, 128, 5), (4, 128, 5)\n",
    "                              (4, 128, 6), (4, 256, 4), (4, 256, 5)]\n",
    "\n",
    "with tqdm(total=nb_tunning_iterations) as pbar:\n",
    "    for d_model in d_modelList:\n",
    "        for d_ff in d_ffList:\n",
    "            for n in nList:\n",
    "                \n",
    "                combination = (d_model, d_ff, n)\n",
    "                if combination not in combinations_already_tested:\n",
    "                    print(f'new combination :{combination}!')\n",
    "\n",
    "                    model_version = f'model_{d_model}_{d_ff}_{n}'\n",
    "                    base_dir = '../tmp/'+model_version+'/'\n",
    "\n",
    "                    if not os.path.exists(base_dir):\n",
    "                        os.mkdir(base_dir)\n",
    "\n",
    "                    chkpt_dir = f'{gamma}_{gae_lambda}_{policy_clip}/'\n",
    "\n",
    "                    if os.path.exists(base_dir+chkpt_dir):\n",
    "                        shutil.rmtree(base_dir+chkpt_dir)\n",
    "                    os.mkdir(base_dir+chkpt_dir)\n",
    "\n",
    "                    params = {'sequence_length':100, 'h':d_model, 'd_k':1, 'd_v':1, 'd_model':d_model, 'd_ff':d_ff, 'n':n, 'rate':0.2, 'rate':0.2}\n",
    "                    agent = Agent(base_dir=base_dir, chkpt_dir=chkpt_dir, transParams=params,\n",
    "                                alpha=0.0001, gamma=gamma, gae_lambda=gae_lambda, policy_clip=policy_clip, batch_size=64, n_epochs=10)\n",
    "\n",
    "                    scores, capitals = PPOtrainingLoop(data, indices_70_percent, agent, n_games=n_games, save_model=True, d_model=4)\n",
    "                    utility.write_image_of_capital_and_scores(base_dir+chkpt_dir+'results.png', capitals, scores)\n",
    "                    print(f\"folder {chkpt_dir} updated!\")\n",
    "                        \n",
    "                else:\n",
    "                    print(f'combination {combination} already tested')\n",
    "\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of the trained and tunned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(1, '../../../../')\n",
    "# from utils import utility\n",
    "from reinforcement_learning.trading_agent.ppo.using_array.ppo_trained_tester_loop_with_idx import PPOtrainedLoop\n",
    "from reinforcement_learning.trading_agent.ppo.agent_no_strat import Agent\n",
    "\n",
    "gamma       = 0.8\n",
    "gae_lambda  = 0.85\n",
    "policy_clip = 0.05\n",
    "\n",
    "d_model = 12\n",
    "d_ff    = 128\n",
    "n       = 4\n",
    "\n",
    "n_games=5000\n",
    "\n",
    "model_version = f'model_{d_model}_{d_ff}_{n}'\n",
    "\n",
    "base_dir = '../tmp/'+model_version+'/'\n",
    "chkpt_dir = f'{gamma}_{gae_lambda}_{policy_clip}/'\n",
    "\n",
    "params = {'sequence_length':100, 'h':d_model, 'd_k':1, 'd_v':1, 'd_model':d_model, 'd_ff':d_ff, 'n':n, 'rate':0.2}\n",
    "agent = Agent(base_dir=base_dir, chkpt_dir=chkpt_dir, transParams=params,\n",
    "            alpha=0.0001, gamma=gamma, gae_lambda=gae_lambda, policy_clip=policy_clip, batch_size=64, n_epochs=10)\n",
    "\n",
    "_ = PPOtrainedLoop(data, indices_30_percent, agent, n_games=1, d_model=d_model) # FIXME: call the model 1 time to builde the layers weights\n",
    "\n",
    "agent.load_weights()\n",
    "\n",
    "results = PPOtrainedLoop(data, indices_30_percent, agent, n_games=n_games, d_model=d_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(results, y=\"capital\", title='Capital over time')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.line(results, y=\"profit\", title='Capital over time')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
